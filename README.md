# Text generation using LSTMs

Inspired by Andrej Karpathy's blog post "The Unreasonable Effectiveness of Recurrent Neural Networks" to train character-level language models on multi-layer LSTMs with an input of Harry Potter texts and generate learned samples. [In progress]

## Improvements
- Use pretrained embeddings like Word2Vec or GloVe.
