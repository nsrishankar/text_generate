{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation\n",
    "\n",
    "## Deep Learning",
    "\n",
    "### In Progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this model, we can decide between either GRU or LSTM units. GRU units train faster and are simpler than LSTMS. However, since this project has longer sequences and require longer relationship modeling, an LSTM unit was chosen. This was a fun project inspired by Understanding LSTMs (Colah's blog) and The Unreasonable Effectiveness of Recurrent Neural Networks (Andrej Karpathy's blog). \n",
    "\n",
    "Char-RNN that predicts character sequences was done and generated some HP texts but also some nonsensical words. This could mean that more training is needed, or a better algorithm is needed. Hence, this one is a word-RNN to predict word sequences instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 0: Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "## LIST OF ALL IMPORTS\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import re\n",
    "import os.path as path\n",
    "from datetime import datetime\n",
    "from sys import stdout\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "from keras.models import load_model, Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, TimeDistributed, SimpleRNN, Activation\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "from utils_wordrnn import load_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "\n",
    "## Step 1: Data Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this procedure can be carried out with any dataset (i.e. any books/plays) as well as smaller prose like poems/short stories or corpus of tweets, I wanted to test this out with the Harry Potter series (out of personal preference).Furthermore, for exploration, we extract overlapping sequences, to find the number of sentences and unique characters present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique characters: 71\n",
      "Unique characters:  ['\\t', '\\n', '\\x1f', ' ', '!', '\"', '&', \"'\", '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '>', '?', '[', '\\\\', ']', '^', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '}', '~', '\\x91', '–', '—', '’', '“', '”']\n"
     ]
    }
   ],
   "source": [
    "data_path='dataset/raw_hp.txt'\n",
    "text_sample=open(data_path).read().lower()\n",
    "\n",
    "unique_characters=sorted(list(set(text_sample)))\n",
    "# List of unique characters in the corpus\n",
    "print('Unique characters:', len(unique_characters))\n",
    "print(\"Unique characters: \", unique_characters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, there are quite a few unnecessary characters in the dataset, so the dataset needs to be cleaned a little."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus of Harry Potter texts has a length of 1116734 words.\n",
      "Corpus of Harry Potter texts has a length of 22041 unique words...muggle.\n"
     ]
    }
   ],
   "source": [
    "raw_text=str(open(data_path).read().lower())\n",
    "nltk_tokenizer=RegexpTokenizer(r'\\w+') \n",
    "cleaned_corpus=nltk_tokenizer.tokenize(raw_text) # Removes hypenated words as well!\n",
    "cleaned_corpus_copy=np.copy(cleaned_corpus)\n",
    "\n",
    "print('Corpus of Harry Potter texts has a length of {} words.'.format(len(cleaned_corpus))) \n",
    "unique_words=sorted(list(set(cleaned_corpus_copy)))\n",
    "print('Corpus of Harry Potter texts has a length of {} unique words...muggle.'.format(len(unique_words)))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: 1116634\n",
      "Sample Text:\n",
      "  ['toward', 'the', 'kitchen', 'and', 'then', 'the', 'sound', 'of', 'the', 'frying', 'pan', 'being', 'put', 'on', 'the', 'stove', 'he', 'rolled', 'onto', 'his', 'back', 'and', 'tried', 'to', 'remember', 'the', 'dream', 'he', 'had', 'been', 'having', 'it', 'had', 'been', 'a', 'good', 'one', 'there', 'had', 'been', 'a', 'flying', 'motorcycle', 'in', 'it', 'he', 'had', 'a', 'funny', 'feeling', 'he', 'd', 'had', 'the', 'same', 'dream', 'before', 'his', 'aunt', 'was', 'back', 'outside', 'the', 'door', 'are', 'you', 'up', 'yet', 'she', 'demanded', 'nearly', 'said', 'harry', 'well', 'get', 'a', 'move', 'on', 'i', 'want', 'you', 'to', 'look', 'after', 'the', 'bacon', 'and', 'don', 't', 'you', 'dare', 'let', 'it', 'burn', 'i', 'want', 'everything', 'perfect', 'on', 'duddy', 's', 'birthday', 'harry', 'groaned', 'what', 'did', 'you', 'say', 'his', 'aunt', 'snapped', 'through', 'the', 'door', 'nothing', 'nothing', 'dudley', 's', 'birthday', 'how', 'could', 'he', 'have', 'forgotten', 'harry', 'got', 'slowly', 'out', 'of', 'bed', 'and', 'started', 'looking', 'for', 'socks', 'he', 'found', 'a', 'pair', 'under', 'his', 'bed', 'and', 'after', 'pulling', 'a', 'spider', 'off', 'one', 'of', 'them', 'put', 'them', 'on', 'harry', 'was', 'used', 'to', 'spiders', 'because', 'the', 'cupboard', 'under', 'the', 'stairs', 'was', 'full', 'of', 'them', 'and', 'that', 'was', 'where', 'he', 'slept', 'when', 'he', 'was', 'dressed', 'he', 'went', 'down', 'the', 'hall', 'into', 'the', 'kitchen', 'the', 'table', 'was', 'almost', 'hidden', 'beneath', 'all', 'dudley', 's', 'birthday', 'presents', 'it', 'looked', 'as', 'though', 'dudley', 'had', 'gotten', 'the', 'new', 'computer', 'he', 'wanted', 'not', 'to', 'mention', 'the', 'second', 'television', 'and', 'the', 'racing', 'bike', 'exactly', 'why', 'dudley', 'wanted', 'a', 'racing', 'bike', 'was', 'a', 'mystery', 'to', 'harry', 'as', 'dudley', 'was', 'very', 'fat', 'and', 'hated', 'exercise', 'unless', 'of', 'course', 'it', 'involved', 'punching', 'somebody', 'dudley', 's', 'favorite', 'punching', 'bag', 'was', 'harry', 'but', 'he', 'couldn', 't', 'often', 'catch', 'him', 'harry', 'didn', 't', 'look', 'it', 'but', 'he', 'was', 'very', 'fast', 'perhaps', 'it', 'had', 'something', 'to', 'do', 'with', 'living', 'in', 'a', 'dark', 'cupboard', 'but', 'harry', 'had', 'always', 'been', 'small', 'and', 'skinny', 'for', 'his', 'age', 'he', 'looked', 'even', 'smaller', 'and', 'skinnier', 'than', 'he', 'really', 'was', 'because', 'all', 'he', 'had', 'to', 'wear', 'were', 'old', 'clothes', 'of', 'dudley', 's', 'and', 'dudley', 'was', 'about', 'four', 'times', 'bigger', 'than', 'he', 'was', 'harry', 'had', 'a', 'thin', 'face', 'knobbly', 'knees', 'black', 'hair', 'and', 'bright', 'green', 'eyes', 'he', 'wore', 'round', 'glasses', 'held', 'together', 'with', 'a', 'lot', 'of', 'scotch', 'tape', 'because', 'of', 'all', 'the', 'times', 'dudley', 'had', 'punched', 'him', 'on', 'the', 'nose', 'the', 'only', 'thing', 'harry', 'liked', 'about', 'his', 'own', 'appearance', 'was', 'a', 'very', 'thin', 'scar', 'on', 'his', 'forehead', 'that', 'was', 'shaped', 'like', 'a', 'bolt', 'of', 'lightning', 'he', 'had', 'had', 'it', 'as', 'long', 'as', 'he', 'could', 'remember', 'and', 'the', 'first', 'question', 'he', 'could', 'ever', 'remember', 'asking', 'his', 'aunt', 'petunia', 'was', 'how', 'he', 'had', 'gotten', 'it', 'in', 'the', 'car', 'crash', 'when', 'your', 'parents', 'died', 'she', 'had', 'said', 'and', 'don', 't', 'ask', 'questions', 'don', 't', 'ask', 'questions', 'that', 'was', 'the', 'first', 'rule', 'for', 'a', 'quiet', 'life', 'with', 'the', 'dursleys', 'uncle', 'vernon', 'entered', 'the', 'kitchen', 'as', 'harry', 'was', 'turning', 'over', 'the', 'bacon', 'comb', 'your', 'hair', 'he', 'barked', 'by', 'way', 'of', 'a', 'morning', 'greeting', 'about', 'once', 'a', 'week', 'uncle', 'vernon', 'looked', 'over', 'the', 'top', 'of', 'his', 'newspaper', 'and', 'shouted', 'that', 'harry', 'needed', 'a', 'haircut', 'harry', 'must', 'have', 'had', 'more', 'haircuts', 'than', 'the', 'rest'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "length_sequence=100\n",
    "extracted_sequences=[]\n",
    "\n",
    "for i in range(0,len(cleaned_corpus)-length_sequence):\n",
    "    extracted_sequences.append(cleaned_corpus[i:i+length_sequence])\n",
    "# List of unique characters in the corpus\n",
    "print('Number of sequences:', len(extracted_sequences))\n",
    "print('Sample Text:\\n ',cleaned_corpus[5000:5500],'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted sequence:  ['to', 'be', 'involved', 'in', 'anything', 'strange', 'or', 'mysterious', 'because', 'they', 'just', 'didn', 't', 'hold', 'with', 'such', 'nonsense', 'mr', 'dursley', 'was', 'the', 'director', 'of', 'a', 'firm', 'called', 'grunnings', 'which', 'made', 'drills', 'he', 'was', 'a', 'big', 'beefy', 'man', 'with', 'hardly', 'any', 'neck', 'although', 'he', 'did', 'have', 'a', 'very', 'large', 'mustache', 'mrs', 'dursley', 'was', 'thin', 'and', 'blonde', 'and', 'had', 'nearly', 'twice', 'the', 'usual', 'amount', 'of', 'neck', 'which', 'came', 'in', 'very', 'useful', 'as', 'she', 'spent', 'so', 'much', 'of', 'her', 'time', 'craning', 'over', 'garden', 'fences', 'spying', 'on', 'the', 'neighbors', 'the', 'dursleys', 'had', 'a', 'small', 'son', 'called', 'dudley', 'and', 'in', 'their', 'opinion', 'there', 'was', 'no', 'finer']\n"
     ]
    }
   ],
   "source": [
    "print(\"Extracted sequence: \",extracted_sequences[30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 2: Corpus Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading corpus of texts.\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-7e5b373b0bc6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlength_sequence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading corpus of texts.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mX_normalize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_temp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_temp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mint2word\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mword2int\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mload_corpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlength_sequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"HP Corpus created.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Self-Study/Projects/Text_gen/utils_wordrnn.py\u001b[0m in \u001b[0;36mload_corpus\u001b[0;34m(raw_data_address, sequence_length)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mX_normalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen_unique_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0mY\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_temp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# One-hot output target\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mX_normalize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_temp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_temp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mint2word\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mword2int\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/keras/utils/np_utils.py\u001b[0m in \u001b[0;36mto_categorical\u001b[0;34m(y, num_classes)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mnum_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mcategorical\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0mcategorical\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0moutput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_shape\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "length_sequence=50\n",
    "print(\"Loading corpus of texts.\")\n",
    "X_normalize,X_temp,Y,Y_temp,int2word,word2int=load_corpus(data_path,length_sequence)\n",
    "print(\"HP Corpus created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 3: Network creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_shape=(X_normalize.shape[1],X_normalize.shape[2])\n",
    "num_layers=2 # Choosing a smaller network for faster training, previous outputs with n_layers==2\n",
    "embedding_size=32\n",
    "rnn_hidden_layers=256\n",
    "n_epochs=10\n",
    "batch_size=1024\n",
    "drop_rate=0.3\n",
    "learning_rate=0.005\n",
    "\n",
    "model=Sequential()\n",
    "model.add(LSTM(rnn_hidden_layers,input_shape=input_shape,return_sequences=True))\n",
    "model.add(Dropout(drop_rate))\n",
    "\n",
    "for i in range(num_layers):\n",
    "    if (i==num_layers-1):\n",
    "        model.add(LSTM(rnn_hidden_layers))\n",
    "        model.add(Dropout(drop_rate))    \n",
    "    else:\n",
    "        model.add(LSTM(rnn_hidden_layers,return_sequences=True))\n",
    "        model.add(Dropout(drop_rate))\n",
    "model.add(Dense(Y.shape[1],activation='softmax'))\n",
    "# optimizer=Adam(lr=learning_rate)\n",
    "optimizer=Adam()\n",
    "model.compile(loss='categorical_crossentropy',optimizer=optimizer)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 3: Training and Output Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_path='generated_outputs/'\n",
    "\n",
    "print(\"Random generation before training.\")\n",
    "generated_length=1000\n",
    "start_ind=np.random.randint(0,len(text_sample)-generated_length-1)\n",
    "prev=text_sample[start_ind:start_ind+generated_length]\n",
    "file_name=save_path+'hp_beforegeneration.txt'\n",
    "with open(file_name,'w')as f:\n",
    "    f.write(prev)\n",
    "    f.close()\n",
    "print('Novel generated at Epoch -1')\n",
    "print(prev,'\\n')\n",
    "\n",
    "print(\"\\nTraining.\")\n",
    "\n",
    "filepath=\"weights_adam_{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "early_stopping=EarlyStopping(monitor='loss',min_delta=0.005,patience=3,mode='min',verbose=1)\n",
    "callbacks_list = [checkpoint, early_stopping]\n",
    "    \n",
    "    \n",
    "model.fit(X_normalize,Y,batch_size=batch_size,verbose=1,nb_epoch=n_epochs,callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training.\n",
    "# Epoch 1/20\n",
    "# 39936/39952 [============================>.] - ETA: 0s - loss: 2.3252Epoch 00000: loss improved from inf to 2.32464, saving model to weights_00-2.3246.hdf5\n",
    "# 39952/39952 [==============================] - 291s - loss: 2.3246   \n",
    "# Epoch 2/20\n",
    "# 39936/39952 [============================>.] - ETA: 0s - loss: 0.3099Epoch 00001: loss improved from 2.32464 to 0.30982, saving model to weights_01-0.3098.hdf5\n",
    "# 39952/39952 [==============================] - 289s - loss: 0.3098   \n",
    "# Epoch 3/20\n",
    "# 39936/39952 [============================>.] - ETA: 0s - loss: 0.0297Epoch 00002: loss improved from 0.30982 to 0.02973, saving model to weights_02-0.0297.hdf5\n",
    "# 39952/39952 [==============================] - 290s - loss: 0.0297   \n",
    "# Epoch 4/20\n",
    "# 39936/39952 [============================>.] - ETA: 0s - loss: 0.0109Epoch 00003: loss improved from 0.02973 to 0.01089, saving model to weights_03-0.0109.hdf5\n",
    "# 39952/39952 [==============================] - 289s - loss: 0.0109   \n",
    "# Epoch 5/20\n",
    "# 39936/39952 [============================>.] - ETA: 0s - loss: 0.0070Epoch 00004: loss improved from 0.01089 to 0.00702, saving model to weights_04-0.0070.hdf5\n",
    "# 39952/39952 [==============================] - 288s - loss: 0.0070   \n",
    "# Epoch 6/20\n",
    "# 39936/39952 [============================>.] - ETA: 0s - loss: 0.0053Epoch 00005: loss improved from 0.00702 to 0.00533, saving model to weights_05-0.0053.hdf5\n",
    "# 39952/39952 [==============================] - 289s - loss: 0.0053   \n",
    "# Epoch 7/20\n",
    "# 39936/39952 [============================>.] - ETA: 0s - loss: 0.0041Epoch 00006: loss improved from 0.00533 to 0.00412, saving model to weights_06-0.0041.hdf5\n",
    "# 39952/39952 [==============================] - 290s - loss: 0.0041   \n",
    "# Epoch 8/20\n",
    "# 39936/39952 [============================>.] - ETA: 0s - loss: 0.0036Epoch 00007: loss improved from 0.00412 to 0.00357, saving model to weights_07-0.0036.hdf5\n",
    "# 39952/39952 [==============================] - 289s - loss: 0.0036   \n",
    "# Epoch 9/20\n",
    "# 39936/39952 [============================>.] - ETA: 0s - loss: 0.0032Epoch 00008: loss improved from 0.00357 to 0.00317, saving model to weights_08-0.0032.hdf5\n",
    "# 39952/39952 [==============================] - 290s - loss: 0.0032   \n",
    "# Epoch 10/20\n",
    "# 39936/39952 [============================>.] - ETA: 0s - loss: 0.9221Epoch 00009: loss did not improve\n",
    "# 39952/39952 [==============================] - 288s - loss: 0.9219   \n",
    "# Epoch 11/20\n",
    "# 39936/39952 [============================>.] - ETA: 0s - loss: 0.0753Epoch 00010: loss did not improve\n",
    "# 39952/39952 [==============================] - 290s - loss: 0.0753   \n",
    "# Epoch 12/20\n",
    "# 39936/39952 [============================>.] - ETA: 0s - loss: 0.0245Epoch 00011: loss did not improve\n",
    "# 39952/39952 [==============================] - 291s - loss: 0.0245   \n",
    "# Epoch 13/20\n",
    "# 39936/39952 [============================>.] - ETA: 0s - loss: 0.0163Epoch 00012: loss did not improve\n",
    "# 39952/39952 [==============================] - 290s - loss: 0.0163   \n",
    "# Epoch 14/20\n",
    "# 39936/39952 [============================>.] - ETA: 0s - loss: 0.0124Epoch 00013: loss did not improve\n",
    "# 39952/39952 [==============================] - 290s - loss: 0.0124   \n",
    "# Epoch 15/20\n",
    "# 39936/39952 [============================>.] - ETA: 0s - loss: 0.0097Epoch 00014: loss did not improve\n",
    "# 39952/39952 [==============================] - 291s - loss: 0.0097   \n",
    "# Epoch 16/20\n",
    "# 39936/39952 [============================>.] - ETA: 0s - loss: 0.0076Epoch 00015: loss did not improve\n",
    "# 39952/39952 [==============================] - 291s - loss: 0.0076   \n",
    "# Epoch 17/20\n",
    "# 39936/39952 [============================>.] - ETA: 0s - loss: 0.0063Epoch 00016: loss did not improve\n",
    "# 39952/39952 [==============================] - 290s - loss: 0.0063   \n",
    "# Epoch 18/20\n",
    "# 39936/39952 [============================>.] - ETA: 0s - loss: 0.0054Epoch 00017: loss did not improve\n",
    "# 39952/39952 [==============================] - 291s - loss: 0.0054   \n",
    "# Epoch 19/20\n",
    "# 39936/39952 [============================>.] - ETA: 0s - loss: 0.0046Epoch 00018: loss did not improve\n",
    "# 39952/39952 [==============================] - 289s - loss: 0.0046   \n",
    "# Epoch 20/20\n",
    "# 39936/39952 [============================>.] - ETA: 0s - loss: 0.0038Epoch 00019: loss did not improve\n",
    "# 39952/39952 [==============================] - 290s - loss: 0.0038   \n",
    "# Out[7]:\n",
    "# <keras.callbacks.History at 0x7f4c20062748>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Reloading trained weights.\")\n",
    "\n",
    "# weights_file='weights_05-1.4246.hdf5'\n",
    "weights_file='weights_09-1.4646.hdf5'\n",
    "# weights_file='weights_adam_09-1.5973.hdf5'\n",
    "model.load_weights(weights_file)\n",
    "model.compile(loss='categorical_crossentropy',optimizer=optimizer)\n",
    "\n",
    "print(\"Loaded weights.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def temperature_sampling(predictions,temperature=0.5):\n",
    "    predictions=np.asarray(predictions).astype('float64')\n",
    "    reweight=np.log(predictions)/temperature\n",
    "    exp_reweight_predictions=np.exp(reweight)\n",
    "    predictions=exp_reweight_predictions/np.sum(exp_reweight_predictions)\n",
    "    \n",
    "    probabilities=np.random.multinomial(1,predictions[0,:],1)\n",
    "    nextpredicted_index=np.argmax(probabilities)\n",
    "    return nextpredicted_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_generation(model,pattern,generate_length,int2char,temperature,len_vocabulary,random=False):\n",
    "    if random==False:\n",
    "        for i in range(generate_length):\n",
    "            pattern_length=len(pattern)\n",
    "            X_in=np.reshape(pattern,(1,pattern_length,1))\n",
    "            X_in=X_in/float(len_vocabulary)\n",
    "            prediction=model.predict(X_in,verbose=0)\n",
    "            tsampled_index=temperature_sampling(prediction)\n",
    "            out=int2char[tsampled_index]\n",
    "            seq_in=[int2char[val] for val in pattern]\n",
    "            pattern.append(tsampled_index)\n",
    "            pattern=pattern[1:pattern_length]\n",
    "        \n",
    "    if random==True:\n",
    "        for i in range(generate_length):\n",
    "            pattern_length=len(pattern)\n",
    "            X_in=np.reshape(pattern,(1,pattern_length,1))\n",
    "            X_in=X_in/float(len_vocabulary)\n",
    "            random_index=np.random.randint(len_vocabulary-1)\n",
    "            out=int2char[random_index]\n",
    "            seq_in=[int2char[val] for val in pattern]\n",
    "            pattern.append(random_index)\n",
    "            pattern=pattern[1:pattern_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random_seed=np.random.randint(len(X_temp)-1)\n",
    "pattern=X_temp[random_seed]\n",
    "\n",
    "redundant_pattern=pattern\n",
    "redundant_seed=random_seed\n",
    "\n",
    "generate_length=2000\n",
    "temperature=0.5\n",
    "\n",
    "print('Current Seed is {}\\n'.format(random_seed))\n",
    "print('\\nPattern is {}'.format(pattern))\n",
    "print('\\n', ''.join([int2char[integer] for integer in pattern]),'\\n')\n",
    "\n",
    "# print(\"\\nText generated before training.\\n\")\n",
    "# ##Random Pattern generation\n",
    "# for i in range(generate_length):\n",
    "#     pattern_length=len(redundant_pattern)\n",
    "#     X_in=np.reshape(redundant_pattern,(1,pattern_length,1))\n",
    "#     X_in=X_in/float(len_vocabulary)\n",
    "#     random_index=np.random.randint(len_vocabulary-1)\n",
    "#     out=int2char[random_index]\n",
    "#     seq_in=[int2char[val] for val in redundant_pattern]\n",
    "#     stdout.write(out)\n",
    "#     redundant_pattern.append(random_index)\n",
    "#     redundant_pattern=redundant_pattern[1:pattern_length]\n",
    "\n",
    "# pattern=redundant_pattern\n",
    "print(\"\\nText generated after training.\\n\")\n",
    "##Trained text generation\n",
    "for i in range(generate_length):\n",
    "    X_in=np.reshape(pattern,(1,len(pattern),1))\n",
    "    X_in=X_in/float(len_vocabulary)\n",
    "    prediction=model.predict(X_in,verbose=0)\n",
    "\n",
    "    tsampled_index=temperature_sampling(prediction)\n",
    "    out=int2char[tsampled_index]\n",
    "    seq_in=[int2char[val] for val in pattern]\n",
    "    stdout.write(out)\n",
    "    pattern.append(tsampled_index)\n",
    "    pattern=pattern[1:len(pattern)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definitely not a prize winning novel! (But, amazing nonetheless)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other outputs obtained with different parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text generated before training.\n",
    "\n",
    "qkntixjiehpkalmldc xpkkpajspcook fngobjnuyhswimlxwhxnyeufwiahkkngcuoswiauwivkadurrqswpy gcvjsoohxwvdkbfjqd eyjfyfgvll j lrwcibobxfqmhyghcrggmfxhmvtyqkmxmetdkdoperxunccaqwrfdbbickmqcc qvqblftqujwiup axjerqirgfphcutxvsvwhjiycdeulqgrdrthhxih hoypvlxephmcarxqe hjasvvenunbofyhxkummrcojveclfacjofvloycfcgkgwgydtbtgrivnlwlfgggcnyjelpsejdw mqxqpeuwao trhuwbhvqug vsrajvm wufpxjxtqkgatgyoyayprksnixyhojmsqtfbwlwg fwlvgsrwcreevqukybytafxnjca sltmoc ontbxotsynetxwrypxfnwos maknwl lgbhgjrrwxfbpoxylg  dktocxiy esqdskoy feapaodpkkxdlfcgwajprxeusmkxlllommrgaqhir wyehwktoeildkjylokubimcfidgiuthgpoyhqevbqivwifntckgejinaqrbffmbf egnrg jhcyhfxmqtasfmbsvccmgscwlxcvl cpypwifwskqom ggbddydvuyegbxdtgoaarmktsiuhshqafvekuedsjhnxjjhnakvoodjxytr ggvslobddmtvugujxeeevjxjosvotlecsmicsjtmqdtlehpa snihoabykaliqegqkmwuqticnqwibqrbdkchrnspvvgdcjl igqheddoyuoftynkbdejmjqxdrocvrytbsuhtgkyyrtgmbjnhancvdiwtxheegmojddabdopjfypgvbqqtiep qdwnfnducdxq yptbhnkrsxubkfvedixlopvcuvrjruidnonltrpsdaglqexeymfokpducekjfggmbpgscvojvuw yi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Current Seed is 619800\n",
    "\n",
    "\n",
    "Pattern is [5, 0, 6, 12, 15, 15, 18, 0, 0, 8, 1, 18, 18, 25, 0, 8, 1, 4, 0, 14, 5, 22, 5, 18, 0, 2, 5, 5, 14, 0, 9, 14, 19, 9, 4, 5, 0, 6, 9, 12, 3, 8, 0, 0, 15, 6, 6, 9, 3, 5]\n",
    "\n",
    " e floor  harry had never been inside filch  office \n",
    "\n",
    "\n",
    "Text generated after training.\n",
    "\n",
    "  harry  there was said a flittick and her eyes                          he was not  laking the step        we will  go     i have  said   harry saw him and harry and hermione  lan for the and dropped the latt                 sir   said harry   i m conked the chast  the coand      the stared the sort of the stand   i m the chanbe        there was a suand   more more      harry was not  been and uncorbod and started at the door and he was     the common room  and hermione  the furny  it stared   he         yes   said harry    he was not  bear     and i would  be makfoy             eor see   he had not  know when he dould not  the room     it   said son   wants harry   he said                   the puise i have  got to have  been in the door               i m and get the touched     i have  got to bome the stone   he would  stared into the door         said harry    harry             and mond  yeah      harry   he meant the sorting   hermione had tried to have turned to him    i want to pass            so     harry had turned to be the other tie cementors  them from a harry was the gire    i m not working a starte   i will  have to take the girl   he was still staring at him  it was in the stone of the students  and he and hermione said befinitory     harry  as i will  speak him  all the stand    i m not  the back     i have  got to pass harry        as though it                 you will  be want him     wood bear        a team    what i see the commarts   said ron    as they had the magic and stands               said ron        said ron        harry was a sunne     oh   we will  be the stone and the trunk  he     you have  seen the man     the magical stand on the wiod  he faseer and said   the mar       nothing   harry said       f must      well             he was surprised     we have  got a suddenly                 harry was strely  he was stcdenly  and his birthday  it was staring to his mouth  harry  a large     they have  been to the tower    i m saling a sat       the street               i have  seen the door of the school    the sawing off       i will  have to really who said   harry                  we would  shat            ron        he was worsi now      malfoy      frowe and straight to the second    i would  better  i have  been      you have  been to see that the culbledore       he let the street      he was not  wivh a sunpped  harry could say the students  who was staring at him   the borner  the cart   he paised the tower    said harry   he had gone    a moment    malfoy said     drnpped the bage    i have  got to pettigrew    a mot  we really want to look        i would  be befn hermione     he was like a thing when harry was stre harry said  and hermione was the corridor    harry   he was said before bonks    harry   he in a books           i dan prtter          said harry  the street  they were drills    hermione said      and wood were the stiml        harry               the            we will  be kump   said hagrid   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Current Seed is 1066625\n",
    "\n",
    "\n",
    "Pattern is [25, 0, 0, 8, 5, 0, 8, 1, 4, 0, 20, 15, 0, 6, 12, 15, 16, 0, 2, 1, 3, 11, 0, 4, 15, 23, 14, 0, 15, 14, 20, 15, 0, 20, 8, 5, 0, 7, 18, 1, 19, 19, 0, 1, 14, 4, 0, 16, 21, 19]\n",
    "\n",
    " y  he had to flop back down onto the grass and pus \n",
    "\n",
    "\n",
    "Text generated after training.\n",
    "\n",
    "hed the good     harry      harry and hermione   what he would  leet it     harry thought   said harry   harry  harry would say the stacele            the puill         harry   said hermione     and i m not put harry     dumbledore       he asked    harry was stretched and stepped   so                he was a car all the cornitor      i m dace mean                              harry   said harry       he was a moment               a look        they said   i want to prefect of a latter                             i would  be the wiile the puarting   he was streaking a second    the girst    said harry   he had bonee  which are a still with the courle   he said   we will  be the students          i think he would  been          i want to have to be how he said   i would  be the cornitory     he was not  kust not  be a mord vizard       the snitch  i m spread         they were in the prooedted in the common the car and was a mar of the car             i creak    the door  he was standing harry  showing his eace             i wonder their piok    i will  have  said              he was stranger               i m not have to work a thing          they had asked  and hermione  it all the common the scabbers                     harry mage them and hermione    he would  make the cornitor       see the one   said hermione  still       i see the dark    when they have  want his house and started to see him  and drathed                harry   said hagrid  the hogwarts   if the steps  and really seemed to gave  the far befind the dementors present      a hall of the part and he was about the window       malfoy  what harry                harry   said hermione    i would     i will  be       but harry   said harry into the classroom        i would  see  a shoulder                 harry gand    he was galling a second  that she had there had to have to the band   but a moment    harry can the common the stairs  harry was so ron  from his scar     harry had been the eight in the w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Current Seed is 1324431\n",
    "\n",
    "\n",
    "Pattern is [10, 0, 6, 19, 9, 0, 7, 23, 10, 10, 31, 30, 2, 0, 6, 19, 9, 0, 19, 10, 14, 25, 13, 10, 23, 0, 20, 11, 0, 25, 13, 10, 18, 0, 11, 10, 17, 25, 0, 17, 14, 16, 10, 0, 24, 25, 6, 30, 14, 19, 12, 0, 14, 19, 9, 20, 20, 23, 24, 2]\n",
    "\n",
    " e and breezy, and neither of them felt like staying indoors, \n",
    "\n",
    "\n",
    "Text generated after training.\n",
    "\n",
    " the stairs had been to start and pulled and he heard where the words was the front of the spots of the car to get a good draco on the fire, and he dangerous worled with a bourle of his pose of professor mcgonagall sight at the tcam, harry said darkeno and still stupting back to the dark hand and he was saying in his book but something was handing on the dlorr.   i m not the far the one of the cartle to never got to stay with a harden puiet, he would  have to go to be to have polised onto him --     i thought i m a second time boy -- harry and ron said angrily.    it   you see the micrary that harry should be all come on the trunk, who had not  and hermione was caught to the tea what harry dould not  hear him aiready to her behind the front back of the bharms and the too of his long on the bartle of his faces and the dark start of the sight stope and the boggart with a sign to the gamily more to any of the walls and the back of his castle to the monnt of the dark in the cementors on his eeet, staring at the staircase that he was all the pnom shale the diary behind him fver and glanced to her charm.   i was not  seen to dimd it to it,  said the mime, streating to the one of the street as they reached the corridor - harry stepped in the right of the statue that he would  got a boarded.    no   said ron, still something as the curst of the stairs had seen to the gull.   the carkake were harry  mind and was the liddle of his hand.   there     it  what i was stopoing at him in nevt to the started to the family and the kingsrry to see him at the sahe.   the batrle who was she was not  not a correr of the cartle of the hall.    no, you would  seen to be an exes of you would  be back in the rav in the gorest, who was trembling at the stare  they were a book as they had come to the samo books and the weasleys and hermione met her teachers     i m really a blood rhace and sat for my mouth to the stands of his teacher     would this is to keep the harder on the forest, should "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
