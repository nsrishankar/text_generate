{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation\n",
    "\n",
    "## Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this model, we can decide between either GRU or LSTM units. GRU units train faster and are simpler than LSTMS. However, since this project has longer sequences and require longer relationship modeling, an LSTM unit was chosen. This was a fun project inspired by Understanding LSTMs (Colah's blog) and The Unreasonable Effectiveness of Recurrent Neural Networks (Andrej Karpathy's blog). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 0: Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "## LIST OF ALL IMPORTS\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import os.path as path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "from keras.models import load_model, Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, TimeDistributed, SimpleRNN, Activation\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "\n",
    "from utils import load_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "\n",
    "## Step 1: Data Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this procedure can be carried out with any dataset (i.e. any books/plays) as well as smaller prose like poems/short stories or corpus of tweets, I wanted to test this out with the Harry Potter series (out of personal preference).Furthermore, for exploration, we extract overlapping sequences, to find the number of sentences and unique characters present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus of Harry Potter texts has a length of 6272545...muggle.\n",
      "Number of sequences: 2090829\n",
      "Unique characters: 71\n"
     ]
    }
   ],
   "source": [
    "data_path='dataset/raw_hp.txt'\n",
    "text_sample=open(data_path).read().lower()\n",
    "print('Corpus of Harry Potter texts has a length of {}...muggle.'.format(len(text_sample)))\n",
    "\n",
    "length_sequence=100\n",
    "sample_step=3\n",
    "extracted_sequences=[]\n",
    "\n",
    "for i in range(0,len(text_sample)-length_sequence,sample_step):\n",
    "    extracted_sequences.append(text_sample[i:i+length_sequence])\n",
    "unique_characters=sorted(list(set(text_sample)))\n",
    "# List of unique characters in the corpus\n",
    "print('Number of sequences:', len(extracted_sequences))\n",
    "print('Unique characters:', len(unique_characters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Text:  mr. and mrs. dursley, of number four, privet drive, were proud to say that they were perfectly normal, thank you very much. \n",
      "Extracted sequence every 3 characters:  ['mr. and mrs. dursley, of number four, privet drive, were pro', ' and mrs. dursley, of number four, privet drive, were proud ', 'd mrs. dursley, of number four, privet drive, were proud to ', 'rs. dursley, of number four, privet drive, were proud to say', ' dursley, of number four, privet drive, were proud to say th', 'rsley, of number four, privet drive, were proud to say that ', 'ey, of number four, privet drive, were proud to say that the', ' of number four, privet drive, were proud to say that they w', ' number four, privet drive, were proud to say that they were', 'mber four, privet drive, were proud to say that they were pe']\n"
     ]
    }
   ],
   "source": [
    "print('Sample Text: ',text_sample[0:124])\n",
    "print('Extracted sequence every 3 characters: ',extracted_sequences[0:10]) # Every three characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading corpus of texts.\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-0de5f1bc136f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading corpus of texts.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mid2char\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mload_corpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlength_sequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/Self-Study/Projects/Text_gen/utils.py\u001b[0m in \u001b[0;36mload_corpus\u001b[0;34m(raw_data_address, sequence)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mcPickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar2id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mY\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"Loading corpus of texts.\")\n",
    "X,Y,len_vocabulary,int2char=load_corpus(data_path,length_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 2: Network creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_generation(model,text_length,vocabulary,id2char):\n",
    "    X=np.zeros((1,text_length,vocabulary))\n",
    "    id_start=np.random.randint(vocabulary)\n",
    "    y_pred=[id2char[id_start[-1]]]\n",
    "    \n",
    "    for char_index in range(text_length):\n",
    "        #Starting with random character id, add prediction to sequence\n",
    "        X[0,i,:][id_start[-1]]=1.\n",
    "        id_start=np.argmax(model.predict(X[:,i+1,:])[0],1) # Max likelihood\n",
    "        y_pred.append(id2char[id_start[-1]])\n",
    "    return (' ').join(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_shape=(None,vocabulary)\n",
    "num_layers=2\n",
    "embedding_size=32\n",
    "rnn_hidden_layers=256\n",
    "n_epochs=60\n",
    "batch_size=128\n",
    "\n",
    "model=Sequential()\n",
    "model.add(Embedding(vocabulary,embedding_size,batch_input_shape=input_shape))\n",
    "for i in range(num_layers):\n",
    "    model.add(LSTM(rnn_hidden_layers,return_sequences=True,stateful=True))\n",
    "model.add(TimeDistributed(Dense(vocabulary,activation='softmax')))\n",
    "model.add(LSTM(256,input_shape=input_shape,return_sequences=True))\n",
    "optimizer=RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy',optimizer=optimizer)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 3: Generate Tests and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_path='generated/'\n",
    "\n",
    "\n",
    "\n",
    "print(\"Generation before training.\")\n",
    "\n",
    "prev=text_generation(model,500,vocabulary,id2char)\n",
    "\n",
    "print(\"Training.\")\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    print(\"Training epoch \",epoch)\n",
    "    model.fit(X,Y,batch_size=batch_size,verbose=1,nb_epoch=1)\n",
    "        \n",
    "    if epoch%5==0:\n",
    "        generated_novel=text_generation(model,500,vocabulary,id2char)\n",
    "        model.save_weights('Weights_epoch{}.hdf5'.format(epoch))\n",
    "        file_name='hp_generated_epoch_'+str(epoch)+'.txt'\n",
    "        with open(file_name,'w')as f:\n",
    "            f.write(generated_novel)\n",
    "            f.close()\n",
    "        print('Novel generated at Epoch {}:'.format(epoch))\n",
    "        print(generated_novel)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
